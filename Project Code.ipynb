{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/gutenberg' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/wandi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f21c0fe1556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wandi/anaconda/lib/python2.7/site-packages/nltk/book.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type: 'texts()' or 'sents()' to list the materials.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'melville-moby_dick.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wandi/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wandi/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/gutenberg' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/wandi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy\n",
    "from nltk.book import *\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "\n",
    "######################################## DATA IMPORT FUNCTION #########################################\n",
    "\n",
    "def corpusSize(corpusname):\n",
    "    corpussize = len(PlaintextCorpusReader(corpusname, '.*').fileids())\n",
    "    return corpussize\n",
    "\n",
    "def listFileName(corpusname, corpussize):\n",
    "    fileName = []\n",
    "    corpusfiles = PlaintextCorpusReader(corpusname, '.*')\n",
    "    corpusfileid = corpusfiles.fileids()\n",
    "    for x in range(corpussize):\n",
    "        fileID = corpusfileid[x]\n",
    "        #.strip('.txt')\n",
    "        fileName.append(fileID)\n",
    "    return fileName\n",
    "\n",
    "def listDirectory(corpusname,corpussize,listName=[]):\n",
    "    fileDirectory = []\n",
    "    for x in range(corpussize):\n",
    "        directory = corpusname + '/' + listName[x]\n",
    "        fileDirectory.append(directory)\n",
    "    return fileDirectory\n",
    "\n",
    "# listDirectory = listDirectory('pd',11 ,listName)\n",
    "\n",
    "def stringCorpus(corpusname,fileDirectory=[]):\n",
    "    corpusstr = \"\"\n",
    "    files = PlaintextCorpusReader(corpusname,'.*')\n",
    "    corpussize = len(files.fileids())\n",
    "    for x in range (1,corpussize):\n",
    "        fileName = open(fileDirectory[x])\n",
    "        fileName = fileName.read()\n",
    "        corpusstr = corpusstr + fileName\n",
    "    return corpusstr\n",
    "\n",
    "def listCorpus(corpusname,fileDirectory=[]):\n",
    "    corpuslist = [[]]\n",
    "    files = PlaintextCorpusReader(corpusname,'.*')\n",
    "    corpussize = len(files.fileids())\n",
    "    for x in range (1,corpussize):\n",
    "        fileName = open(fileDirectory[x])\n",
    "        fileName = fileName.read()\n",
    "        ######## TOKENIZE: #######\n",
    "        corpuslist.append(re.split('\\W+',fileName))\n",
    "    return corpuslist\n",
    "\n",
    "def toLower(corpussize, corpuslist = []):\n",
    "    corpuslower = [[]]\n",
    "    for i in range (1,corpussize):\n",
    "        file = corpuslist[i]\n",
    "        file = [w.lower() for w in file]\n",
    "        corpuslower.append(file)\n",
    "    return corpuslower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpusSize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58258166f6c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpdischarge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcorpussize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpusSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdischarge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfileList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistFileName\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpdischarge\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpussize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdirectoryList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdischarge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpussize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpusSize' is not defined"
     ]
    }
   ],
   "source": [
    "###################################### IMPORT PD & P4R CORPUS ########################################\n",
    "\n",
    "pdischarge='pd'\n",
    "corpussize = corpusSize(pdischarge)\n",
    "fileList = listFileName (pdischarge,corpussize)\n",
    "directoryList = listDirectory(pdischarge, corpussize, fileList)\n",
    "corpusList = listCorpus(pdischarge, directoryList)\n",
    "corpusString = stringCorpus(pdischarge, directoryList)\n",
    "pd = corpusList\n",
    "pd = toLower(corpussize,pd)\n",
    "\n",
    "# print (pd[1])\n",
    "\n",
    "p4rname = 'p4r'\n",
    "p4rsize = corpusSize(p4rname)\n",
    "p4rName = listFileName(p4rname,p4rsize)\n",
    "p4rDirectory = listDirectory(p4rname, p4rsize, p4rName)\n",
    "p4rCorpus = listCorpus(p4rname, p4rDirectory)\n",
    "p4r = toLower(p4rsize,p4rCorpus)\n",
    "\n",
    "# print (p4r[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################## PATIENT DE-INDENTIFICATION FUNCTION ############################################\n",
    "# two kinds of patient ID extractors were used to achieve the proper index because the corpus of discharge reports\n",
    "# were written in 2 different types of formats\n",
    "\n",
    "############## FIRST TYPE OF DISCHARGE REPORT (FORMAT 1)###########\n",
    "def listOfID1 (beginning, end, corpuslist = [],idList = [[]]):\n",
    "    for x in range (beginning, end):\n",
    "        file = corpuslist[x]\n",
    "        aindex = file.index('patient')\n",
    "        zindex = file.index(\"author\")\n",
    "        pID = []\n",
    "        pID = file[aindex:zindex]\n",
    "        idList.append(pID)\n",
    "    return idList\n",
    "\n",
    "############## SECOND TYPE OF DISCHARGE REPORT (FORMAT 2)###########\n",
    "def listOfID2 (beginning, end, corpuslist = [],idList = [[]]):\n",
    "    for x in range (beginning, end):\n",
    "        file = corpuslist[x]\n",
    "        aindex = file.index('patient')\n",
    "        zindex = file.index(\"diagnoses\") #different stop index\n",
    "        pID = []\n",
    "        pID = file[aindex:zindex-1]\n",
    "        idList.append(pID)\n",
    "    return idList\n",
    "\n",
    "####### NEW CORPUS WITH PATIENT INFO DE-IDED #######\n",
    "def deIdentify (corpussize, patientInfo = [], corpuslist = []):\n",
    "    dIDed = []\n",
    "    for i in range (corpussize):\n",
    "        result= list(filter(lambda x: x not in patientInfo[i], corpuslist[i]))\n",
    "        dIDed.append(result)\n",
    "    return dIDed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5e1e4e81a5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#PD CORPUS DE-ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpdIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpdIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistOfID1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpdIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistOfID2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpdIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistOfID1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpussize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpdIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#PD CORPUS DE-ID\n",
    "pdIDs = [[]]\n",
    "pdIDs = listOfID1(1,5,pd,pdIDs)\n",
    "pdIDs = listOfID2(5,9,pd,pdIDs)\n",
    "pdIDs = listOfID1(9,corpussize,pd,pdIDs)\n",
    "corpusDeIDed = deIdentify(corpussize, pdIDs, pd)\n",
    "\n",
    "# for x in range (1,corpussize):\n",
    "# print (pdIDs[2], \"-----------------------\")\n",
    "\n",
    "#P4R CORPUS De-ID\n",
    "\n",
    "p4ID= pdIDs[4]\n",
    "p4rDeIDed = [[]]\n",
    "for i in range (1,p4rsize):\n",
    "    result = list(filter(lambda x: x not in p4ID, p4r[i]))\n",
    "    p4rDeIDed.append(result)\n",
    "\n",
    "# print(len(p4rDeIDed))\n",
    "print (\"Total of \", len(p4ID), \" tokens associated with patient id were extracted, sample of patient MR report after de-identification: \\n \\n\",\n",
    "       p4rDeIDed[1][140:200]) #sample selection of data after de-identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpussize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-83eceb5c85b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpdStemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpussize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mstemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusDeIDed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpusDeIDed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpdStemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpussize' is not defined"
     ]
    }
   ],
   "source": [
    "################### STEMMING (optional) ###################\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def stemmer(filelength,file = []):\n",
    "    stemmedfile = []\n",
    "    wnstem = WordNetLemmatizer()\n",
    "    for i in range (filelength):\n",
    "        stem = wnstem.lemmatize(file[i])\n",
    "        stemmedfile.append(stem)\n",
    "    return stemmedfile\n",
    "\n",
    "pdStemmed = [[]]\n",
    "\n",
    "for x in range (1,corpussize):\n",
    "    stemmed = stemmer(len(corpusDeIDed[x]),corpusDeIDed[x])\n",
    "    pdStemmed.append(stemmed)\n",
    "\n",
    "print (corpusDeIDed[1][30:120],\"\\n\",pdStemmed[1][30:120]) # does not seem to do anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## PLUMBING ##################\n",
    "import re \n",
    "\n",
    "# remove non-semantic tokens\n",
    "pdwoNum = [s for s in p4rDeIDed[1] if not re.search(r'\\d',s)]\n",
    "\n",
    "# remove plumbing words (see discussion)\n",
    "plumbing = []\n",
    "fdist = FreqDist(pdwoNum) #file = corpuslist[x]\n",
    "for x in range (0,len(pdwoNum)):\n",
    "    if (fdist[pdwoNum[x]]>=15 and pdwoNum[x] not in plumbing):\n",
    "        plumbing.append(pdwoNum[x])\n",
    "print (\"Total of \", len(plumbing), \" plumbing words extracted. Sample of plumbing words: \\n\", plumbing[1:10]) #selection sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-284e0ae6e215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m################### BI/TRIGRAMS ######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrigramCollocationFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdwoNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "################### BI/TRIGRAMS ######################\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(pdwoNum) \n",
    "finder.apply_freq_filter(2)\n",
    "ignored_words = plumbing #filter out plumbing words\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "print(\"top 10 pairs of trigrams:\")\n",
    "finder.nbest(trigram_measures.pmi, 20) \n",
    "#next step: filter using tags / categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'pd/p01d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d79f76eff97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpd01\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pd/p01d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpd01\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'pd/p01d.txt'"
     ]
    }
   ],
   "source": [
    "################# K MEANS CLUSTER ######################\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import string\n",
    "import collections\n",
    "\n",
    "pd01=open('pd/p01d.txt')\n",
    "pd01=pd01.read()\n",
    "\n",
    "def processFile(file):\n",
    "    fileTokened = tokenizer.tokenize(file)\n",
    "    fileLowered = [w.lower() for w in fileTokened]\n",
    "    fileLength = len(fileLowered)\n",
    "    stemmedfile = []\n",
    "    wnstem = WordNetLemmatizer()\n",
    "    for i in range (fileLength):\n",
    "        stem = wnstem.lemmatize(file[i])\n",
    "        stemmedfile.append(stem)\n",
    "    return stemmedfile\n",
    "\n",
    "def cluster_texts(texts, clusters=3):\n",
    "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "    vectorizer = TfidfVectorizer(tokenizer=processFile,\n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 max_df=0.9,\n",
    "                                 min_df=0,\n",
    "                                 lowercase=True)\n",
    " \n",
    "    tfidf_model = vectorizer.fit_transform(texts)\n",
    "    km_model = KMeans(n_clusters=clusters)\n",
    "    km_model.fit(tfidf_model)\n",
    "    order_centroids = km_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(clusters):\n",
    "        print (\"top 10 items in cluster %d:\" % i)\n",
    "        for ind in order_centroids[i,:10]:\n",
    "            print (\" %s\"% terms[ind])\n",
    "    clustering = collections.defaultdict(list)\n",
    " \n",
    "    for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    " \n",
    "    return clustering\n",
    " \n",
    "###### PRINTING THING ######\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    articles = pd01\n",
    "    n = 3\n",
    "    clusters = cluster_texts(articles, n)\n",
    "#     print(dict(clusters))\n",
    "    for i in range (n):\n",
    "        print(\"cluster\",i, \":\" ,len(clusters[i]) ,\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6c29350787b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "############################### LDA MODEL ###################################\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# read files\n",
    "txt1 = open('p4r/p4mr1.txt')\n",
    "\n",
    "mr1str = txt1.read()\n",
    "\n",
    "txt2 = open('p4r/p4mr2.txt')\n",
    "\n",
    "mr2str = txt2.read()\n",
    "\n",
    "txt3 = open('p4r/p4mr3.txt')\n",
    "\n",
    "mr3str = txt3.read()\n",
    "\n",
    "txt4 = open('p4r/p4mr4.txt')\n",
    "\n",
    "mr4str = txt4.read()\n",
    "\n",
    "txt5 = open('p4r/p4mr5.txt')\n",
    "\n",
    "mr5str = txt5.read()\n",
    "\n",
    "# create sample documents\n",
    "doc_a = mr1str\n",
    "doc_b = mr2str\n",
    "doc_c = mr3str\n",
    "doc_d = mr4str\n",
    "doc_e = mr5str\n",
    "\n",
    "# load tokenizer \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = stopwords.words('english')\n",
    "\n",
    "# Create plumbing words list\n",
    "pw = plumbing\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "        \n",
    "# loop through document list\n",
    "texts = []\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # patient de-ID\n",
    "    deID_tokens = list(filter(lambda x: x not in p4ID, tokens))\n",
    "    \n",
    "    # remove plumbing words\n",
    "    plumbed_tokens = [i for i in deID_tokens if not i in pw]\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in plumbed_tokens if not i in en_stop]\n",
    "    \n",
    "    # remove numbers (non-semantic) tokens\n",
    "    denummed_tokens = [s for s in stopped_tokens if not re.search(r'\\d',s)]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(denummed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7fcf882848bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wandi/anaconda/lib/python2.7/site-packages/nltk/tokenize/regexp.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "############################################### MISC. TESTERS #########################################################\n",
    "\n",
    "############# CONVERT TO LOWER CASE ####################\n",
    "\n",
    "def toLower(fizesize, file = []):\n",
    "    fl = []\n",
    "    fl = [w.lower() for w in file]\n",
    "    return fl\n",
    "\n",
    "\n",
    "########### REMOVE PUNCTUATIONS ############\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize(file)\n",
    "\n",
    "\n",
    "######## FILTER OUT STOP WORDS ###############\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "######## LOCATE PLUMBING WORDS ###########\n",
    "plumbing = []\n",
    "fdist = FreqDist(file) #file = corpuslist[x]\n",
    "for x in range (0,n):\n",
    "    if (fdist[file[x]]>=5):\n",
    "        plumbing.append(file[x])\n",
    "    \n",
    "######## FILTER OUT PLUMBING/STOP WORDS ########\n",
    "fileFiltered = filter(lambda x: x not in plumbing, file)\n",
    "\n",
    "############# STEMMING ########################\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def stemmer(filelength,file = []):\n",
    "    stemmedfile = []\n",
    "    wnstem = WordNetLemmatizer()\n",
    "    for i in range (filelength):\n",
    "        stem = wnstem.lemmatize(file[i])\n",
    "        stemmedfile.append(stem)\n",
    "    return stemmedfile\n",
    "\n",
    "\n",
    "mrlowered = toLower(len(mrfilter),mrfilter)\n",
    "mrstemmed = stemmer(len(mrfilter),mrlowered)\n",
    "print (mrstemmed)\n",
    "\n",
    "#################### BIGRAMS ##########################\n",
    "finder = BigramCollocationFinder.from_words(mrkwo)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "sorted(bigram for bigram, score in scored) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
